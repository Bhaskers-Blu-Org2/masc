{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "The benchmark setup used a 1,000-node Accumulo 2.0.0 Cluster (16,000 cores) running and a 256-node Spark 2.4.3 cluster (4,096 cores). All nodes used [Azure D16s_v3](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-general) (16 cores) virtual machines.\n",
    "\n",
    "In all experiments we use the same base dataset which is a collection of Twitter user tweets with labeled sentiment value. This dataset is known as the Sentiment140 dataset ([Go, Bhayani, & Huang, 2009](http://www-nlp.stanford.edu/courses/cs224n/2009/fp/3.pdf)). The training data consist of 1.6M samples of tweets, where each tweet has columns indicating the sentiment label, user, timestamp, query term, and text. The text is limited to 140 characters and the overall uncompressed size of the training dataset is 227MB.\n",
    "\n",
    "| sentiment | id | date | query_string | user | text |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "|0|1467810369|Mon Apr 06 22:19:...|    NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
    "|0|1467810672|Mon Apr 06 22:19:...|    NO_QUERY|  scotthamilton|is upset that he ...|\n",
    "|0|1467810917|Mon Apr 06 22:19:...|    NO_QUERY|       mattycus|@Kenichan I dived...|\n",
    "\n",
    "To evaluate different table sizes and the impact of splitting the following procedure was used to generate the Accumulo tables:\n",
    "\n",
    "- Prefix id with split keys (e.g. 0000, 0001, ..., 1024)\n",
    "- Create Accumulo table and configure splits\n",
    "- Upload prefixed data to Accumulo using Spark and the MASC writer \n",
    "- Duplicate data using custom Accumulo server-side iterator\n",
    "- Validate data partitioning\n",
    "\n",
    "A common machine learning scenario was evaluated using a sentiment model trained using [SparkML](https://spark.apache.org/docs/latest/ml-guide.html). \n",
    "To train the classification model, we generated feature vectors from the text of tweets (text column). We used a feature engineering pipeline (a.k.a. featurizer) that breaks the text into tokens, splitting on whitespaces and discarding any capitalization and non-alphabetical characters. The pipeline consisted of \n",
    "\n",
    "- Regex Tokenizer\n",
    "- Hashing Transformer\n",
    "- Logistic Regression\n",
    "\n",
    "To run the notebook, you need to first download `protobuf-java-3.5.1.jar` from [here](https://search.maven.org/artifact/com.google.protobuf/protobuf-java/3.5.1/bundle) and include the jar file in `/home/rba1/.m2/repository/com/google/protobuf/protobuf-java/3.5.1/` folder. Then, start a toree kernel\n",
    "\n",
    "```bash\n",
    "JAR=\"file:///home/rba1/webscale-ai-test/lib/accumulo-spark-datasource-1.0.0-SNAPSHOT-shaded.jar\"\n",
    "jupyter toree install \\\n",
    "    --replace \\\n",
    "    --user \\\n",
    "    --kernel_name=accumulo \\\n",
    "    --spark_home=${SPARK_HOME} \\\n",
    "    --spark_opts=\"--master yarn --jars $JAR \\\n",
    "        --packages org.apache.spark:spark-avro_2.11:2.4.3,ml.combust.mleap:mleap-spark_2.11:0.14.0 \\\n",
    "        --driver-memory 16g \\\n",
    "        --executor-memory 12g \\\n",
    "        --driver-cores 4 \\\n",
    "        --executor-cores 4 \\\n",
    "        --num-executors 256\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataUrl = https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
       "dataDir = data\n",
       "dataFilename = training.1600000.processed.noemoticon.csv\n",
       "splitSizes = Array(2, 4, 12, 102, 16, 32, 128, 1024, 160, 320, 1280, 10240, 1600, 3200, 12800, 16384)\n",
       "dataSizes = Array(100GB, 100GB, 100GB, 100GB, 1T, 1T, 1T, 1T, 10T, 10T, 10T, 10T, 100T, 100T, 100T, 1PB)\n",
       "dataGB = Array(100, 100, 100, 100, 1024, 1024, 1024, 1024, 10240, 10240, 10240, 10240, 102400, 102400, 102400, 1048576)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(100, 100, 100, 100, 1024, 1024, 1024, 1024, 10240, 10240, 10240, 10240, 102400, 102400, 102400, 1048576)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataUrl = \"https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\"\n",
    "val dataDir = \"data\"\n",
    "val dataFilename = \"training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "val splitSizes = Array(2,     4,    12,    102,\n",
    "                       16,    32,   128,   1024,\n",
    "                       160,   320,   1280,  10240, \n",
    "                       1600,  3200,  12800, \n",
    "                       16384)\n",
    "\n",
    "val dataSizes = Array(\"100GB\", \"100GB\", \"100GB\", \"100GB\",\n",
    "                      \"1T\", \"1T\", \"1T\", \"1T\",\n",
    "                      \"10T\", \"10T\", \"10T\", \"10T\",\n",
    "                      \"100T\", \"100T\", \"100T\",\n",
    "                      \"1PB\")\n",
    "\n",
    "val dataGB = Array(100, 100, 100, 100,              // 100GB\n",
    "                   1024, 1024, 1024, 1024,          // 1TB\n",
    "                   10240, 10240, 10240, 10240,      // 10TB\n",
    "                   102400, 102400, 102400,          // 100TB\n",
    "                   1024*1024)                       // 1PB\n",
    "\n",
    "import sys.process._\n",
    "import java.net.URL\n",
    "import java.io._\n",
    "import java.util.zip.{GZIPOutputStream, ZipFile}\n",
    "import java.nio.file.{Files, Path, Paths, StandardCopyOption}\n",
    "import scala.collection.JavaConverters._\n",
    "import scala.io.Source\n",
    "import org.apache.accumulo.core.client.Accumulo\n",
    "import org.apache.hadoop.io.Text\n",
    "import java.util.Calendar\n",
    "import java.util.Base64\n",
    "import java.net.URL\n",
    "import com.google.common.io.Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Twitter data and unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tmpZipFile = <lazy>\n",
       "zipFile = <lazy>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<lazy>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create output directory\n",
    "new File(dataDir).mkdirs\n",
    "\n",
    "lazy val tmpZipFile = Paths.get(dataDir, \"tmp.zip\").toFile\n",
    "\n",
    "new URL(dataUrl) #> tmpZipFile !!\n",
    "\n",
    "lazy val zipFile = new ZipFile(tmpZipFile)\n",
    "for (entry <- zipFile.entries.asScala)\n",
    "  Files.copy(zipFile.getInputStream(entry), \n",
    "             Paths.get(dataDir).resolve(entry.getName),\n",
    "             StandardCopyOption.REPLACE_EXISTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"0\",\"1467810369\",\"Mon Apr 06 22:19:45 PDT 2009\",\"NO_QUERY\",\"_TheSpecialOne_\",\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"\n",
      "\"0\",\"1467810672\",\"Mon Apr 06 22:19:49 PDT 2009\",\"NO_QUERY\",\"scotthamilton\",\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\"\n",
      "\"0\",\"1467810917\",\"Mon Apr 06 22:19:53 PDT 2009\",\"NO_QUERY\",\"mattycus\",\"@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds\"\n",
      "\"0\",\"1467811184\",\"Mon Apr 06 22:19:57 PDT 2009\",\"NO_QUERY\",\"ElleCTF\",\"my whole body feels itchy and like its on fire \"\n",
      "\"0\",\"1467811193\",\"Mon Apr 06 22:19:57 PDT 2009\",\"NO_QUERY\",\"Karoli\",\"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainDataSize = 1600000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "getTrainData: ()Iterator[String]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1600000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTrainData() = Source.fromFile(new File(dataDir, dataFilename), \"ISO-8859-1\").getLines\n",
    "\n",
    "val trainDataSize = getTrainData.size\n",
    "\n",
    "// Show input data header\n",
    "for (line <- getTrainData.take(5)) {\n",
    "    println(line)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Twitter Data with Row ID Prefixes\n",
    "\n",
    "With the prefixes, we can easily split Accumulo tables used for holding replicated twitter data and speed up the process of writing data into these tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sentiment140_prefix_2.csv.gz...\n",
      "Creating sentiment140_prefix_4.csv.gz...\n",
      "Creating sentiment140_prefix_12.csv.gz...\n",
      "Creating sentiment140_prefix_102.csv.gz...\n",
      "Creating sentiment140_prefix_16.csv.gz...\n",
      "Creating sentiment140_prefix_32.csv.gz...\n",
      "Creating sentiment140_prefix_128.csv.gz...\n",
      "Creating sentiment140_prefix_1024.csv.gz...\n",
      "Creating sentiment140_prefix_160.csv.gz...\n",
      "Creating sentiment140_prefix_320.csv.gz...\n",
      "Creating sentiment140_prefix_1280.csv.gz...\n",
      "Creating sentiment140_prefix_10240.csv.gz...\n",
      "Creating sentiment140_prefix_1600.csv.gz...\n",
      "Creating sentiment140_prefix_3200.csv.gz...\n",
      "Creating sentiment140_prefix_12800.csv.gz...\n",
      "Creating sentiment140_prefix_16384.csv.gz...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "prepareData: (splits: Int)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepareData(splits: Int) {\n",
    "    import org.apache.hadoop.fs.FileSystem\n",
    "    import org.apache.hadoop.conf.Configuration;\n",
    "\n",
    "    val conf = new Configuration()\n",
    "    val fs = FileSystem.get(conf)\n",
    "    \n",
    "    val divider = Math.ceil(trainDataSize / splits.toDouble).toInt\n",
    "\n",
    "    val digits = (Math.log10(splits) + 1).toInt\n",
    "\n",
    "    val outputFilename = s\"sentiment140_prefix_${splits}.csv.gz\"\n",
    "    \n",
    "    println(s\"Creating ${outputFilename}...\")\n",
    "    \n",
    "    val output = new PrintWriter(\n",
    "        new GZIPOutputStream(\n",
    "             fs.create(new org.apache.hadoop.fs.Path(outputFilename))))\n",
    "\n",
    "    var idx = 0\n",
    "    for (line <- getTrainData) {\n",
    "        // it's a bit crude, but fast\n",
    "        val sep = line.indexOf(\"\\\",\\\"\") + 3\n",
    "        val f0 = line.substring(0, sep)\n",
    "        val f1 = line.substring(sep)\n",
    "\n",
    "        output.print(f0)\n",
    "        output.print(s\"%0${digits}d-\".format(idx / divider))\n",
    "        output.println(f1)\n",
    "\n",
    "        idx += 1\n",
    "    }\n",
    "\n",
    "    output.close\n",
    "}\n",
    "\n",
    "for (s <- splitSizes)\n",
    "    prepareData(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data to Accumulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "val conf = new SparkConf()\n",
    "conf.setAppName(\"AccumuloBenchmark\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"3g\")\n",
    "\n",
    "new SparkContext(conf)\n",
    "\n",
    "println(\"Spark version %s\".format(sc.version))\n",
    "println(\"Scala %s\".format(util.Properties.versionString))\n",
    "println\n",
    "sc.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROPS_PATH = /home/centos/webscale-ai-test/conf/accumulo-client.properties\n",
       "sqlContext = org.apache.spark.sql.SQLContext@5b860512\n",
       "schema = StructType(StructField(sentiment,StringType,true), StructField(prefix,StringType,true), StructField(date,StringType,true), StructField(query_string,StringType,true), StructField(user,StringType,true), StructField(text,StringType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(sentiment,StringType,true), StructField(prefix,StringType,true), StructField(date,StringType,true), StructField(query_string,StringType,true), StructField(user,StringType,true), StructField(text,StringType,true))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{LongType, DoubleType, StringType, StructField, StructType}\n",
    "import scala.collection.JavaConverters._\n",
    "\n",
    "// client property file path\n",
    "val PROPS_PATH = \"/home/centos/webscale-ai-test/conf/accumulo-client.properties\"\n",
    "\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "val schema = StructType(Array(\n",
    "    StructField(\"sentiment\", StringType),\n",
    "    StructField(\"prefix\", StringType),\n",
    "    StructField(\"date\", StringType),\n",
    "    StructField(\"query_string\", StringType),\n",
    "    StructField(\"user\", StringType),\n",
    "    StructField(\"text\", StringType)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+--------------------+------------+---------------+--------------------+\n",
      "|sentiment|      prefix|                date|query_string|           user|                text|\n",
      "+---------+------------+--------------------+------------+---------------+--------------------+\n",
      "|        0|0-1467810369|Mon Apr 06 22:19:...|    NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|        0|0-1467810672|Mon Apr 06 22:19:...|    NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|        0|0-1467810917|Mon Apr 06 22:19:...|    NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|        0|0-1467811184|Mon Apr 06 22:19:...|    NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|        0|0-1467811193|Mon Apr 06 22:19:...|    NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|        0|0-1467811372|Mon Apr 06 22:20:...|    NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|        0|0-1467811592|Mon Apr 06 22:20:...|    NO_QUERY|        mybirch|         Need a hug |\n",
      "|        0|0-1467811594|Mon Apr 06 22:20:...|    NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|        0|0-1467811795|Mon Apr 06 22:20:...|    NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|        0|0-1467812025|Mon Apr 06 22:20:...|    NO_QUERY|        mimismo|@twittera que me ...|\n",
      "+---------+------------+--------------------+------------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "split = 4\n",
       "df = [sentiment: string, prefix: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[sentiment: string, prefix: string ... 4 more fields]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val split = 4\n",
    "val df = spark.read\n",
    "    .schema(schema)\n",
    "    .csv(s\"sentiment140_prefix_${split}.csv.gz\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pre-split Accumulo tables and upload prefixed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "client = org.apache.accumulo.core.clientImpl.ClientContext@559a972f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.accumulo.core.clientImpl.ClientContext@559a972f"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val client = Accumulo.newClient().from(PROPS_PATH).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ( (split, label) <- splitSizes.zip(dataSizes)) {\n",
    "    val tableName = s\"twitter_${split}_${label}\"\n",
    "    val df = spark.read\n",
    "                  .schema(schema)\n",
    "                  .csv(s\"sentiment140_prefix_${split}.csv.gz\")\n",
    "                  .repartition(128)\n",
    "                  .cache()\n",
    "\n",
    "    val splitValues = df.selectExpr(\"split(prefix, '_')[0]\").distinct().collect()\n",
    "    val splits = new java.util.TreeSet(\n",
    "                splitValues\n",
    "                  .map { _(0).toString }\n",
    "                  .sorted\n",
    "                  .drop(1) // exclude the first partion as it's an upper bound\n",
    "                  .map(new Text(_))\n",
    "                  .toSeq\n",
    "                  .asJava)\n",
    "\n",
    "    val now = Calendar.getInstance().getTime()\n",
    "    \n",
    "    println(s\"${now} | number of splits: ${splits.size()} for table ${tableName} ${splits.first()} to ${splits.last()}\")\n",
    "    // delete if exists\n",
    "    // if (client.tableOperations.exists(tableName))\n",
    "    //    client.tableOperations.delete(tableName)\n",
    "    \n",
    "    // re-create\n",
    "    client.tableOperations.create(tableName)\n",
    "    \n",
    "    // add the splits\n",
    "    client.tableOperations.addSplits(tableName, splits)\n",
    "\n",
    "    val props = Accumulo.newClientProperties().from(PROPS_PATH).build().asScala\n",
    "\n",
    "    props.put(\"rowkey\", \"prefix\")\n",
    "    props.put(\"table\", tableName)\n",
    "\n",
    "    df.write.format(\"com.microsoft.accumulo\").options(props).save()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate data in Accumulo\n",
    "As it's easier to parallelize and monitor we're running the duplication from the command line.\n",
    "\n",
    "### Script for a single table\n",
    "\n",
    "Create the following script file\n",
    "\n",
    "duplicate-twitter-data.sh\n",
    "\n",
    "```bash\n",
    "#!/bin/sh\n",
    "export TABLE=$1\n",
    "export DUPS=$2\n",
    "export ASHELL='/opt/muchos/install/accumulo-2.0.0/bin/accumulo shell -u root -p secret'\n",
    "\n",
    "{\n",
    "printf \"_\\n$DUPS\\n\" | $ASHELL -e \"setiter -n dup -class org.apache.accumulo.iterator.DuplicationIterator -p 10 -majc -t $TABLE\"\n",
    "\n",
    "$ASHELL -e \"listiter -t $TABLE -majc\"\n",
    "time $ASHELL -e \"compact -w -t $TABLE\"\n",
    "\n",
    "$ASHELL -e \"deleteiter -n dup -majc -t $TABLE\"\n",
    "} > twitter_${TABLE}_${DUPS}.stdout.log 2>twitter_${TABLE}_${DUPS}.stderr.log\n",
    "```\n",
    "\n",
    "Execute the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./duplicate-twitter-data.sh twitter_2_100GB 452 &\n",
      "./duplicate-twitter-data.sh twitter_4_100GB 452 &\n",
      "./duplicate-twitter-data.sh twitter_12_100GB 452 &\n",
      "./duplicate-twitter-data.sh twitter_102_100GB 452 &\n",
      "./duplicate-twitter-data.sh twitter_16_1T 4620 &\n",
      "./duplicate-twitter-data.sh twitter_32_1T 4620 &\n",
      "./duplicate-twitter-data.sh twitter_128_1T 4620 &\n",
      "./duplicate-twitter-data.sh twitter_1024_1T 4620 &\n",
      "./duplicate-twitter-data.sh twitter_160_10T 46193 &\n",
      "./duplicate-twitter-data.sh twitter_320_10T 46193 &\n",
      "./duplicate-twitter-data.sh twitter_1280_10T 46193 &\n",
      "./duplicate-twitter-data.sh twitter_10240_10T 46193 &\n",
      "./duplicate-twitter-data.sh twitter_1600_100T 461928 &\n",
      "./duplicate-twitter-data.sh twitter_3200_100T 461928 &\n",
      "./duplicate-twitter-data.sh twitter_12800_100T 461928 &\n",
      "./duplicate-twitter-data.sh twitter_16384_1PB 4730141 &\n"
     ]
    }
   ],
   "source": [
    "// generate calls to duplicate all table/settings\n",
    "for ( ((split, label), size) <- splitSizes.zip(dataSizes).zip(dataGB)) {\n",
    "    val dataSizeInMB = new File(dataDir, dataFilename).length / (1024 * 1024)\n",
    "    \n",
    "    val sourceSize = dataSizeInMB / 1024.0\n",
    "    val dup = Math.ceil(size / sourceSize).toInt\n",
    "    println(s\"./duplicate-twitter-data.sh twitter_${split}_${label} ${dup} &\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+--------------------+------------+---------------+--------------------+-----+\n",
      "|sentiment|      prefix|                date|query_string|           user|                text|label|\n",
      "+---------+------------+--------------------+------------+---------------+--------------------+-----+\n",
      "|        0|0-1981018684|Sun May 31 08:13:...|    NO_QUERY|     sherilynne|Drinking coffee a...|  0.0|\n",
      "|        0|0-2013384590|Tue Jun 02 22:32:...|    NO_QUERY|  hannahjarin29|home. sleep. I lo...|  0.0|\n",
      "|        0|0-2183989225|Mon Jun 15 15:15:...|    NO_QUERY|     ariannexxx|@wilsonswar I hop...|  0.0|\n",
      "|        0|0-2192370750|Tue Jun 16 06:56:...|    NO_QUERY|     briancbray|I went from havin...|  0.0|\n",
      "|        4|1-2067142866|Sun Jun 07 11:49:...|    NO_QUERY| AaronxFlavored|Deep fried Cajun ...|  1.0|\n",
      "|        0|0-2017827506|Wed Jun 03 09:00:...|    NO_QUERY|        ruthijo|@westernslopetix ...|  0.0|\n",
      "|        0|0-2058667150|Sat Jun 06 15:23:...|    NO_QUERY|      Tried100X|The Polish restau...|  0.0|\n",
      "|        0|0-2176093566|Mon Jun 15 02:53:...|    NO_QUERY|jasmineadarling|Uuugh @KevinFierc...|  0.0|\n",
      "|        0|0-2198076199|Tue Jun 16 15:34:...|    NO_QUERY|      ___Andrew|Requesting Solari...|  0.0|\n",
      "|        4|1-2062962303|Sun Jun 07 00:53:...|    NO_QUERY|        Ynachan|Anyway I do miss ...|  1.0|\n",
      "|        4|1-1833584619|Sun May 17 23:45:...|    NO_QUERY|         dashpr|@imeldamatt Of co...|  1.0|\n",
      "|        4|1-2189368254|Mon Jun 15 23:52:...|    NO_QUERY|        Dyddles|Is watching Bedti...|  1.0|\n",
      "|        4|1-2190082146|Tue Jun 16 01:42:...|    NO_QUERY|  Samantha_Kang|i am soo tired. g...|  1.0|\n",
      "|        4|1-1970427938|Sat May 30 03:49:...|    NO_QUERY|        harryyx|@mileycyrus http:...|  1.0|\n",
      "|        4|1-2050581872|Fri Jun 05 18:55:...|    NO_QUERY|  sockmonkeymax|Max is leaving in...|  1.0|\n",
      "|        0|0-2047066971|Fri Jun 05 13:06:...|    NO_QUERY|    tameraclark|@SQLRockstar OH m...|  0.0|\n",
      "|        0|0-1983289665|Sun May 31 12:52:...|    NO_QUERY|   scenexxqueen|Thinks it sucks t...|  0.0|\n",
      "|        0|0-2234450314|Thu Jun 18 23:04:...|    NO_QUERY|  themoofactory|liked the idea of...|  0.0|\n",
      "|        4|1-1960351949|Fri May 29 07:49:...|    NO_QUERY|backstreet_team|@ClaudeKelly YEAH...|  1.0|\n",
      "|        0|0-1964030109|Fri May 29 13:37:...|    NO_QUERY|     emokermit7|Sucky day so far ...|  0.0|\n",
      "+---------+------------+--------------------+------------+---------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "train_df = [sentiment: string, prefix: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[sentiment: string, prefix: string ... 5 more fields]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "\n",
    "// some data prep (shuffle + label creation)\n",
    "var train_df = spark.read.schema(schema).csv(\"sentiment140_prefix_2.csv.gz\")\n",
    "    .orderBy(rand(42))\n",
    "    .withColumn(\"label\", when($\"sentiment\".cast(IntegerType) > 0, 1.0).otherwise(0.0))\n",
    "    .cache()\n",
    "    \n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer = regexTok_9452d8f54c06\n",
       "hashingTF = hashingTF_890807d8c228\n",
       "lr = logreg_5944f231dbd7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "logreg_5944f231dbd7"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{CountVectorizer, RegexTokenizer, HashingTF}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import scala.math.pow\n",
    "\n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setGaps(false)\n",
    "  .setPattern(\"\\\\p{L}+\")\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"words\")\n",
    "\n",
    "val hashingTF = new HashingTF()\n",
    "  .setInputCol(\"words\")\n",
    "  .setOutputCol(\"features\")\n",
    "  .setNumFeatures(pow(2, 18).toInt)\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "  .setMaxIter(1)\n",
    "  .setRegParam(0.2)\n",
    "  .setElasticNetParam(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train lr model: 19.738933929s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lr_pipeline = pipeline_d15a9ccfcd10\n",
       "t0 = 6688427813206030\n",
       "lrModel = pipeline_d15a9ccfcd10\n",
       "t1 = 6688447552139959\n",
       "train_time = 19.738933929\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "19.738933929"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr_pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, lr))\n",
    "\n",
    "var t0 = System.nanoTime()\n",
    "\n",
    "val lrModel = lr_pipeline.fit(train_df)\n",
    "\n",
    "var t1 = System.nanoTime()\n",
    "val train_time = (t1 - t0)*1e-9\n",
    "println(\"Time to train lr model: \" + train_time + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize Model using MLeap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to a bundle file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "train_df_persist = [sentiment: string, prefix: string ... 5 more fields]\n",
       "sbc = SparkBundleContext(Some([sentiment: string, prefix: string ... 10 more fields]),BundleRegistry(scala.tools.nsc.interpreter.IMain$TranslatingClassLoader@101bd174))\n",
       "bundleFilePath = /tmp/twitter.model.lr.zip\n",
       "fileObj = /tmp/twitter.model.lr.zip\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/tmp/twitter.model.lr.zip"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// MLeap/Bundle.ML Serialization Libraries\n",
    "import ml.combust.mleap.spark.SparkSupport._\n",
    "import ml.combust.bundle.BundleFile\n",
    "import org.apache.spark.ml.bundle.SparkBundleContext\n",
    "import resource._\n",
    "import java.io.File\n",
    "\n",
    "val train_df_persist = train_df.persist()\n",
    "\n",
    "// Serialize model pipeline to bundle.ml\n",
    "val sbc = SparkBundleContext().withDataset(lrModel.transform(train_df_persist))\n",
    "\n",
    "val bundleFilePath = \"/tmp/twitter.model.lr.zip\"\n",
    "val fileObj = new File(bundleFilePath)\n",
    "if (fileObj.exists()){\n",
    "    fileObj.delete()\n",
    "    println(\"Deleted an existed model bundle file.\")\n",
    "}\n",
    "for(bf <- managed(BundleFile(\"jar:file:\" + bundleFilePath))) {\n",
    "    lrModel.writeBundle.save(bf)(sbc).get\n",
    "    println(\"Saved model to a bundle file.\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate SparkML inference perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy val numQueryThreads = \"8\"\n",
    "\n",
    "for (maxPartitions <- Array(\"20480\")) {\n",
    "    for ((split, size) <- splitSizes.zip(dataSizes)) {\n",
    "        val TEST_TABLE_NAME = s\"twitter_${split}_${size}\"\n",
    "\n",
    "        println(s\"LOG: $size with $split splits for table $TEST_TABLE_NAME using $numQueryThreads threads and $maxPartitions spark partitions\")\n",
    "\n",
    "        val props = Accumulo.newClientProperties().from(PROPS_PATH).build()\n",
    "        props.put(\"table\", TEST_TABLE_NAME)\n",
    "        props.put(\"rowKey\", \"id\")\n",
    "        props.put(\"maxPartitions\", maxPartitions)\n",
    "        props.put(\"numQueryThreads\", numQueryThreads)\n",
    "\n",
    "        {\n",
    "            var t0 = System.nanoTime()\n",
    "\n",
    "            val df = spark.read\n",
    "                        .format(\"com.microsoft.accumulo\")\n",
    "                        .options(props.asScala)\n",
    "                        .schema(schema)\n",
    "                        .load()\n",
    "\n",
    "            val cnt = lrModel\n",
    "                .transform(df)\n",
    "                .filter(\"prediction > 0.9\")\n",
    "                .count()\n",
    "\n",
    "            val time = (System.nanoTime() - t0)*1e-9\n",
    "\n",
    "            println(s\"DATA-INFER-0.9\\t$size\\t$split\\t$numQueryThreads\\t$time\\t$cnt\\t$maxPartitions\")\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Accumulo server-side inference performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mleapBundle = <lazy>\n",
       "mleapBundleBase64 = <lazy>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<lazy>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy val mleapBundle = Resources.toByteArray(new URL(\"file:///tmp/twitter.model.lr.zip\"))\n",
    "lazy val mleapBundleBase64 = Base64.getEncoder().encodeToString(mleapBundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy val numQueryThreads = \"8\"\n",
    "\n",
    "for ((split, size) <- splitSizes.zip(dataSizes)) {\n",
    "{\n",
    "    val TEST_TABLE_NAME = s\"twitter_${split}_${size}\"\n",
    "    \n",
    "    println(s\"LOG: $size with $split splits for table $TEST_TABLE_NAME\")\n",
    "    \n",
    "    val props = Accumulo.newClientProperties().from(PROPS_PATH).build()\n",
    "    props.put(\"table\", TEST_TABLE_NAME)\n",
    "    props.put(\"rowKey\", \"id\")\n",
    "    // override the 200 default so it scales to the full number of executors we have\n",
    "    props.put(\"maxPartitions\", \"2048\")\n",
    "    props.put(\"numQueryThreads\", numQueryThreads)\n",
    "        \n",
    "    // count the data\n",
    "    {\n",
    "        var t0 = System.nanoTime()\n",
    "    \n",
    "        var cnt = spark.read\n",
    "                    .format(\"com.microsoft.accumulo\")\n",
    "                    .options(props.asScala)\n",
    "                    .schema(schema)\n",
    "                    .load()\n",
    "                    .count()\n",
    "\n",
    "        val time = (System.nanoTime() - t0)*1e-9\n",
    "\n",
    "        println(s\"DATA-COUNT: $size,$split,$numQueryThreads,$time,$cnt\")\n",
    "    }\n",
    "    \n",
    "    // server-side inference with 30% data transfer\n",
    "    {\n",
    "        var t0 = System.nanoTime()\n",
    "    \n",
    "        props.put(\"mleap\", mleapBundleBase64)\n",
    "        props.put(\"mleapfilter\", \"${prediction > .9}\")\n",
    "        \n",
    "        var cnt = spark.read\n",
    "                    .format(\"com.microsoft.accumulo\")\n",
    "                    .options(props.asScala)\n",
    "                    .schema(schema)\n",
    "                    .load()\n",
    "                    .count()\n",
    "\n",
    "        val time = (System.nanoTime() - t0)*1e-9\n",
    "\n",
    "        println(s\"DATA-INFER-0.9: $size,$split,$numQueryThreads,$time,$cnt\")\n",
    "    }\n",
    "    \n",
    "    // server-side inference with no data transfer\n",
    "    {\n",
    "        var t0 = System.nanoTime()\n",
    "    \n",
    "        props.put(\"mleap\", mleapBundleBase64)\n",
    "        props.put(\"mleapfilter\", \"${false}\")\n",
    "        \n",
    "        var cnt = spark.read\n",
    "                    .format(\"com.microsoft.accumulo\")\n",
    "                    .options(props.asScala)\n",
    "                    .schema(schema)\n",
    "                    .load()\n",
    "                    .count()\n",
    "\n",
    "        val time = (System.nanoTime() - t0)*1e-9\n",
    "\n",
    "        println(s\"DATA-INFER-0.0: $size,$split,$numQueryThreads,$time,$cnt\")\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accumulo - Scala",
   "language": "scala",
   "name": "accumulo_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
